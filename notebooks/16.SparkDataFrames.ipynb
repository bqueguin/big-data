{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "- Enable wider audiences beyond “Big Data” engineers to leverage the power of distributed processing\n",
    "- Inspired by data frames in R and Python (Pandas)\n",
    "- Designed from the ground-up to support modern big\n",
    "data and data science applications\n",
    "- Extension to the existing RDD API\n",
    "\n",
    "## References\n",
    "- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n",
    "- [PySpark Cheat Sheet: Spark DataFrames in Python](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames are :\n",
    "- The preferred abstraction in Spark\n",
    "- Strongly typed collection of distributed elements \n",
    "- Built on Resilient Distributed Datasets (RDD)\n",
    "- Immutable once constructed\n",
    "\n",
    "### With Dataframes you can :\n",
    "- Track lineage information to efficiently recompute lost data \n",
    "- Enable operations on collection of elements in parallel\n",
    "\n",
    "### You construct DataFrames\n",
    "- by parallelizing existing collections (e.g., Pandas DataFrames) \n",
    "- by transforming an existing DataFrames\n",
    "- from files in HDFS or any other storage system (e.g., Parquet)\n",
    "\n",
    "### Features\n",
    "- Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster\n",
    "- Support for a wide array of data formats and storage systems\n",
    "- Seamless integration with all big data tooling and infrastructure via Spark\n",
    "- APIs for Python, Java, Scala, and R\n",
    "\n",
    "### DataFrames versus RDDs\n",
    "- Nice API for new users familiar with data frames in other programming languages.\n",
    "- For existing Spark users, the API will make Spark easier to program than using RDDs\n",
    "- For both sets of users, DataFrames will improve performance through intelligent optimizations and code-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Spark shell:**\n",
    "\n",
    "~~~ bash\n",
    "pyspark\n",
    "~~~\n",
    "\n",
    "Output similar to the following will be displayed, followed by a `>>>` REPL prompt:\n",
    "\n",
    "~~~\n",
    "Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\n",
    "[GCC 7.2.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "2018-09-18 17:13:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "~~~\n",
    "\n",
    "Read data and convert to Dataset\n",
    "\n",
    "~~~ py\n",
    "df = sqlContext.read.csv(\"/tmp/irmar.csv\", sep=';', header=True)\n",
    "~~~\n",
    "\n",
    "~~~\n",
    ">>> df2.show()\n",
    "+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n",
    "|_c0|                name|       phone|office|organization|position|  hdr|    team1|   team2|\n",
    "+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n",
    "|  0|      Alphonse Paul |+33223235223|   214|          R1|     DOC|False|      EDP|      NA|\n",
    "|  1|        Ammari Zied |+33223235811|   209|          R1|      MC| True|      EDP|      NA|\n",
    ".\n",
    ".\n",
    ".\n",
    "| 18|    Bernier Joachim |+33223237558|   214|          R1|     DOC|False|   ANANUM|      NA|\n",
    "| 19|   Berthelot Pierre |+33223236043|   601|          R1|      PE| True|       GA|      NA|\n",
    "+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n",
    "only showing top 20 rows\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations, Actions, Laziness\n",
    "\n",
    "Like RDDs, DataFrames are lazy. Transformations contribute to the query plan, but they don't execute anything.\n",
    "Actions cause the execution of the query.\n",
    "\n",
    "### Transformation examples\n",
    "- filter\n",
    "- select\n",
    "- drop\n",
    "- intersect \n",
    "- join\n",
    "### Action examples\n",
    "- count \n",
    "- collect \n",
    "- show \n",
    "- head\n",
    "- take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "| firstname| lastname|      login|\n",
      "+----------+---------+-----------+\n",
      "|      Uzel|    Simon|     uzel_d|\n",
      "|   Perrine|   Moreau|   moreau_p|\n",
      "|     Elise|    Negri|    negri_e|\n",
      "|   Camille|   Cochet|   cochet_c|\n",
      "|   Nolwenn| Giguelay| giguelay_n|\n",
      "|     Youen|    Meyer|    meyer_y|\n",
      "|    Emilie|  Lacoste|  lacoste_e|\n",
      "|       Pia|  LeBihan|  lebihan_p|\n",
      "|      Yann|    Evain|    evain_y|\n",
      "|   Camille|    Guyon|    guyon_c|\n",
      "|  Mathilde|  LeMener|  lemener_m|\n",
      "|    Gildas| LeGuilly| liguilly_g|\n",
      "|    Pierre| Gardelle| gardelle_p|\n",
      "|Christophe|Boulineau|boulineau_c|\n",
      "|      Omar| Aitichou| aitichou_o|\n",
      "|     Lijun|      Chi|      chi_l|\n",
      "|    Jiawei|      Liu|      lin_j|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "# The following three lines are not necessary\n",
    "# in the pyspark shell\n",
    "conf = SparkConf().setAppName(\"people\").setMaster(\"local[*]\") \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.json(\"/tmp/people.json\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
